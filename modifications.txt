[colossalai > booster > plugin > dp_plugin_base.py]
Line 8: Add from colossalai.utils import DataParallelSampler
Line 54: Comment sampler = DistributedSampler(~)
         Add sampler = DataParallelSampler(dataset, shuffle=shuffle)

[colossalai > nn > parallel > layers > linear.py]
Totally modified to distinguish model setting

[colossalai > zero > gemini > placement_policy.py]
Line 35: need_mem_stats: bool = True
Line 52: need_mem_stats: bool = True

[colossalai > zero > gemini > chunk > search_utils.py]
Line 164-172: Divide cases - 1) dp_degree == 1 for TP
                             2) dp_degree == 8 for DP

[colossalai > zero > gemini > chunk > chunk.py]
Line 156-157: # fix_gather
              self.fix_gather = False
Line 363-364: else:
                  self.fix_gather = True
Line 373: if self.is_gathered:  ->  if self.is_gathered and not self.fix_gather:

[transformers > trainer.py]
Line 2286-2287: import GPUtil
                logs[“gpu_mem”] = GPUtil.getGPUs()[0].memoryUsed

[colossalai > nn > parallel > layers > module_utils.py]
Line 103-111: Avoid the case of bias=False
Line 114-115: Commented to avoid additional sharding
Line 119-120: Commented

[colossalai > zero > gemini > colo_init_context.py]
Line 33: not_default_dist_spec=False
Line 51-59: if not not_default_dist_spec:
                Indent existing codes
Line 129-134: bias -> do not apply tensor parallelism
              ‘Linear’ or ‘Embedding’ -> apply tensor parallelism
              Others -> do not apply tensor parallelism


