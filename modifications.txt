[colossalai > booster > plugin > dp_plugin_base.py]
Line 8: Add from colossalai.utils import DataParallelSampler
Line 54: Comment sampler = DistributedSampler(~)
         Add sampler = DataParallelSampler(dataset, shuffle=shuffle)

[colossalai > nn > _ops > embedding.py]
Totally modified

[colossalai > nn > _ops > linear.py]
Totally modified

[colossalai > nn > parallel > layers > linear.py]
Totally modified to distinguish model setting

[colossalai > zero > gemini > placement_policy.py]
Line 35: need_mem_stats: bool = True
Line 52: need_mem_stats: bool = True

[colossalai > zero > gemini > chunk > search_utils.py]
Line 199-204: if dp_degree > 1:
                  # Original
                  config_dict[dp_degree] = dict(chunk_size=best_chunk_size, keep_gathered=False)
              else:
                  # Avoid all_gather and reduce_scatter for 1D TP
                  config_dict[dp_degree] = dict(chunk_size=best_chunk_size, keep_gathered=True)

[transformers > trainer.py]
Line 2649-2651: Check the used GPU memory in the limited condition

[colossalai > nn > parallel > layers > module_utils.py]
Line 103-111: Avoid the case of bias=False
Line 119-120: Commented

[colossalai > zero > gemini > colo_init_context.py]
Line 74-77: Some arguments are added to experiments
Line 93-95: Added arguments take values
Line 133-145: Convert model param to coloparam depending on the param name and experimental arguments

[colossalai > booster > plugin > gemini_plugin.py]
Line 43: , tp_degree: int
Line 49-55: if tp_degree == 1:
                state_dict = model.state_dict(only_rank_0=True)
            if self.coordinator.is_master():
                save_state_dict(state_dict, checkpoint, use_safetensors)
            else:
                state_dict = model.state_dict(only_rank_0=False)
                save_state_dict(state_dict, checkpoint, use_safetensors)

[colossalai > booster > booster.py]
Line 187: tp_degree: int = 1
Line 200: , tp_degree=tp_degree

[colossalai > checkpoint_io > checkpoint_io_base.py]
Line 109: tp_degree: int = 1
Line 143: , tp_degree=tp_degree
Line 238: , tp_degree: int

